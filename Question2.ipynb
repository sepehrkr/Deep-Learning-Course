{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sepehrkr/Deep-Learning-Course/blob/main/Question2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVADt2C9Mzfz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6CteSLUIjY2Y"
      },
      "outputs": [],
      "source": [
        "import hazm\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\n",
        "\n",
        "from IPython import display\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "05BRLVwRHew5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mCjUs8pNjnBW"
      },
      "outputs": [],
      "source": [
        "normalizer = hazm.Normalizer(persian_numbers=False)\n",
        "normalize = lambda text: normalizer.normalize(text)\n",
        "\n",
        "with open('./ferdousi.txt','r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = data.splitlines()[2:]\n",
        "if len(data) % 2 != 0:\n",
        "  data = data[:-1]\n",
        "data = np.array(data)\n",
        "df = pd.DataFrame()\n",
        "df['first beyt'] = data[range(0,len(data),2)]\n",
        "df['second beyt'] = data[range(1,len(data),2)]\n",
        "df['first beyt'] = df['first beyt'].apply(normalize)\n",
        "df['second beyt'] = df['second beyt'].apply(normalize)\n",
        "df['texts'] = df['first beyt'] + \"<|startoftext|>\" + df['second beyt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8gA7a9ipSxI"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token='<BOS>',\n",
        "    eos_token='<EOS>',\n",
        "    pad_token='<PAD>',\n",
        "    unk_token='<UNK>'\n",
        ")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '<BOS>',\n",
        "    \"eos_token\": '<EOS>',\n",
        "    \"pad_token\": '<PAD>',\n",
        "    \"unk_token\": '<UNK>'\n",
        "})\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token_id=tokenizer(\"<BOS>\")[\"input_ids\"][0],\n",
        "    eos_token_id=tokenizer(\"<EOS>\")[\"input_ids\"][0],\n",
        "    pad_token_id=tokenizer(\"<PAD>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<UNK>\")[\"input_ids\"][0],\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/gpt2/\")\n",
        "config.save_pretrained(\"/content/gpt2/\")\n",
        "\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\" -P /content/gpt2/\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    './gpt2/',\n",
        "    bos_token='<BOS>',\n",
        "    eos_token='<EOS>',\n",
        "    pad_token='<PAD>',\n",
        "    unk_token='<UNK>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset"
      ],
      "metadata": {
        "id": "ZT56GjukHioe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2wPXuhmxtiaj"
      },
      "outputs": [],
      "source": [
        "class PoetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, tokenizer, max_length=1024):\n",
        "\n",
        "        self.tokenizer = tokenizer  # the gpt2 tokenizer we instantiated\n",
        "        self.encoding = []\n",
        "        self.mask = []\n",
        "\n",
        "        for text in texts:\n",
        "          encoding_dict = tokenizer('<BOS>' + text + '<EOS>',truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "          self.encoding.append(torch.tensor(encoding_dict['input_ids']))\n",
        "          self.mask.append(torch.tensor(encoding_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoding)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoding[idx], self.mask[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "X-1WUtHSRHwC"
      },
      "outputs": [],
      "source": [
        "max_length = 30\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dataset = PoetDataset(df['texts'], tokenizer, max_length)\n",
        "trainset, testset = random_split(dataset, [0.8, 0.2])\n",
        "\n",
        "trainLoader = DataLoader(trainset, batch_size, shuffle=True)\n",
        "testLoader = DataLoader(testset, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "Cu_wV1ySHlnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nntb6KQzUMqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ef1d56-08e9-47d6-8e3b-30f5cd96f586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "epochs_num = 3\n",
        "warmup_num = 100\n",
        "total_steps = len(trainLoader) * epochs_num\n",
        "\n",
        "configuration = GPT2Config.from_pretrained('./gpt2', output_hidden_states=False)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2\", config=configuration)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr=5e-4, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_num, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fy8Oju0AdwlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e94cf63-bea2-4666-8a67-1000d63c81f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 201/1241 [02:46<24:48,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: سر کودک مرده بینی چو شیر\n",
            "کجا آمد سر بدگمان\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 401/1241 [05:26<13:42,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: به آزادگان گفت یزدان سپاس\n",
            "که از ما من این سخن‌های جز از من نگفت\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 601/1241 [08:01<11:01,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: ببردش دمان تا به البرز کوه\n",
            "بیاری یکی را یکی کنده گروه\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 801/1241 [10:46<07:37,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: شنیدی که ضحاک شد ناسپاس\n",
            "برین سان همی برگذشتند سال\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1001/1241 [13:21<02:51,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: نبینم کسی کز پی نام و ننگ\n",
            "بدست آورد آن را پلنگ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1201/1241 [16:02<00:20,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: که یزدان ستایش نخواهد همی\n",
            "مگر با تو یزدان آهرمنی\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241/1241 [16:32<00:00,  1.25it/s]\n",
            "100%|██████████| 1241/1241 [05:32<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.6723771737910968\n",
            "epoch 1: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 201/1241 [02:44<17:58,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: به پشت هیون چمان برنشست\n",
            "به دست اندرون ترگ و رومی کلاه\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 401/1241 [05:27<13:04,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: بفرمود تا پیشش ایرانیان\n",
            "ببستند و بگشاد بند میان\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 601/1241 [08:06<07:42,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: به نخچیر شد شهریار جهان\n",
            "بیامد ز روم و بگرفت نهان\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 801/1241 [10:44<05:10,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: چو خورشید برزد سر از برج شیر\n",
            "ز هامون و دشت برخاست غو\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1001/1241 [13:28<03:39,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: پر از دردم‌ای پهلوان از دو روی\n",
            "چو خورشید تابان بنمود روی\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1201/1241 [15:59<00:24,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: بیامد ورا تنگ در بر گرفت\n",
            "ازو ماند اندر شگفت و شگفت\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241/1241 [16:29<00:00,  1.25it/s]\n",
            "100%|██████████| 1241/1241 [05:23<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.3053331181082775\n",
            "epoch 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 201/1241 [02:40<12:34,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: یکی آبگیرست زان روی شهر\n",
            "ز خون بر شده دشت چون زرد و چو گل\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 401/1241 [05:19<08:04,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: هم آنکس که بودند پا از دوال\n",
            "گرفتند گردان دوال دوال کمر\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 601/1241 [07:59<10:19,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: سپهبد چو آمد به نزدیک گرگ\n",
            "به پیکار آن اژدهای سترگ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 801/1241 [10:36<05:55,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: نگنجد تو را این سخن در خرد\n",
            "که مغز وی از خرد برخورد\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1001/1241 [13:18<02:20,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: بفرمود تا پرده برداشتند\n",
            "ز درگاهشان شاد بگذاشتند\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1201/1241 [15:51<00:26,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output: چو آگه شد از کار خاقان چین\n",
            "که سالارشان داد بر پشت زین\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241/1241 [16:24<00:00,  1.26it/s]\n",
            "100%|██████████| 1241/1241 [05:18<00:00,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.0684753616042526\n",
            "training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_stats = []\n",
        "\n",
        "for epoch in range(epochs_num):\n",
        "\n",
        "    print(f'epoch {epoch}: ')\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (encoding, mask) in tqdm(enumerate(trainLoader), total=len(trainLoader)):\n",
        "\n",
        "        encoding = encoding.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(encoding, labels=encoding, attention_mask=mask, token_type_ids=None)\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Get sample every 100 batches.\n",
        "        if i % 200 == 0 and not i == 0:\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            first_beyt = df.loc[np.random.randint(0, len(df))]['first beyt']\n",
        "            sample_input = first_beyt + \"<|startoftext|>\"\n",
        "            sample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\n",
        "            sample_input_ids = sample_input_ids.to(device)\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                input_ids=sample_input_ids,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                max_length=50,\n",
        "                top_p=0.95,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
        "                gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n",
        "                gen_sample_output = gen_sample_output.replace(\"<BOS>\", \"\")\n",
        "                gen_sample_output = gen_sample_output.replace(\"<EOS>\", \"\")\n",
        "                gen_sample_output = gen_sample_output.replace(\"<UNK>\", \"\")\n",
        "\n",
        "                print(f'Example output: {gen_sample_output}')\n",
        "\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(trainLoader)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for (encoding, mask) in tqdm(testLoader, total=len(testLoader)):\n",
        "\n",
        "        encoding = encoding.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(encoding, attention_mask=mask, labels=encoding)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(testLoader)\n",
        "\n",
        "    print(f'Validation loss: {avg_val_loss}')\n",
        "\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print('training finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "SWaDj9P2Hqho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "total_test_loss = 0\n",
        "perplexity = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "for (encoding, mask) in tqdm(testLoader, total=len(testLoader)):\n",
        "\n",
        "    encoding = encoding.to(device)\n",
        "    mask = mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encoding, attention_mask=mask, labels=encoding)\n",
        "        loss = outputs[0]\n",
        "\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "avg_val_loss = total_eval_loss / len(testLoader)\n",
        "perplexity = 2**(-avg_val_loss)\n",
        "\n",
        "print(f'Validation loss: {avg_val_loss}')\n",
        "print(f'Perplexity: {perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjVj3rfIDDGa",
        "outputId": "4135522a-ec50-4e0d-bb77-7dd168c463e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 311/311 [01:50<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 5.481922217504005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = 2**(-avg_val_loss)\n",
        "print(f'Perplexity: {perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_nEmMVzGBQD",
        "outputId": "925e2ab1-5fde-45fd-b421-bfafc0022172"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 0.022375717929265255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Perplexity** is 0.0223, thats not good becasue to the limited resoucrces we trained for 3 epochs."
      ],
      "metadata": {
        "id": "ro-8DDwRHAie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(20):\n",
        "    first_beyt = df.loc[np.random.randint(0, len(df))]['first beyt']\n",
        "    sample_input = first_beyt + \"<|startoftext|>\"\n",
        "    sample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\n",
        "    sample_input_ids = sample_input_ids.to(device)\n",
        "\n",
        "    sample_outputs = model.generate(\n",
        "        input_ids=sample_input_ids,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        max_length=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
        "        gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n",
        "        gen_sample_output = gen_sample_output.replace(\"<BOS>\", \"\")\n",
        "        gen_sample_output = gen_sample_output.replace(\"<EOS>\", \"\")\n",
        "        gen_sample_output = gen_sample_output.replace(\"<UNK>\", \"\")\n",
        "\n",
        "        print(f'Example output {j+1}:\\n{gen_sample_output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2pQgOvMB-52",
        "outputId": "f3126f54-7137-4787-d9df-638f185cb617"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example output 1:\n",
            "کند حلقه در گردن کنگره\n",
            "چو آن مهره بر گردن کنگره\n",
            "Example output 2:\n",
            "نماند آنگهی جایگاه سخن\n",
            "به پالیز کین بخت گم کرد بن\n",
            "Example output 3:\n",
            "در و دشت گفتی که زرین شدست\n",
            "بران سان که گفتی ستاره شدست\n",
            "Example output 4:\n",
            "کمند اندرافگند و برگاشت روی\n",
            "جهان شد چو چشم خروس آبگون\n",
            "Example output 5:\n",
            "جدا گشت زو کودکی چون پری\n",
            "بدید آن بزرگی و فر و اورند شاه\n",
            "Example output 6:\n",
            "سپهبد فریبرز را گفت مرد\n",
            "که فردا ز گردان ما باد سرد\n",
            "Example output 7:\n",
            "تنش پرنگار از کران تا کران\n",
            "همان با درفش سران افسران\n",
            "Example output 8:\n",
            "یکی رزم کرد آن نه بر آرزوی\n",
            "که شاهد برو جامه خسروی\n",
            "Example output 9:\n",
            "خروشان همی رفت نیزه بدست\n",
            "همانا چو پیلی شد از پیل مست\n",
            "Example output 10:\n",
            "بگفت این و باد از جگر برکشید\n",
            "زمین کوه گشت آهنین شد ناپدید\n",
            "Example output 11:\n",
            "جهاندیده گیو اندر آمد به آب\n",
            "جهان کرد ازو دید چون پر آفتاب\n",
            "Example output 12:\n",
            "به دستان نگه کرد فرخنده سام\n",
            "ابا او به سام دلیر و جوان\n",
            "Example output 13:\n",
            "زن و مرد ایرانیان صدهزار\n",
            "همه گرد و شایسته کارزار\n",
            "Example output 14:\n",
            "بیامد بدرگاه سالار نو\n",
            "به نزدیک بهرام یل خوار نو\n",
            "Example output 15:\n",
            "نجویی همی ناله بوق را\n",
            "چه آوای رامشگران با تو را\n",
            "Example output 16:\n",
            "رسیدند نزدیک قیصر فراز\n",
            "بفرمود تا بازگردد به راز\n",
            "Example output 17:\n",
            "فرود آمد و شاه برپای خاست\n",
            "سخنها بسی کرد و پاسخ بسیچ\n",
            "Example output 18:\n",
            "سیم روز داراب کردند نام\n",
            "دمان اندران شاه لهراسپ کام\n",
            "Example output 19:\n",
            "همی گفت کز دادگر کردگار\n",
            "نباشد شگفتست شاه و سوار\n",
            "Example output 20:\n",
            "چو روز دگر شاه نوشین روان\n",
            "بگردید و دیدش ورا پهلوان\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Bad!"
      ],
      "metadata": {
        "id": "nz6r-RbpHOcJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHDa9j9NXFjMeprJtAEQ17",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
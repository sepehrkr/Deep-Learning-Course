{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "4UttKnz0gngm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(x:torch.Tensor, y:torch.Tensor):\n",
        "    x_real = x.clone()\n",
        "    y_onehot_real = torch.zeros((x.shape[0],10),dtype=x.dtype,device=x.device)\n",
        "    batches = torch.arange(x.shape[0])\n",
        "    y_onehot_real[batches,y] = 1.0\n",
        "    x_real[batches,:10] = y_onehot_real\n",
        "\n",
        "    x_fake = x.clone()\n",
        "    s = torch.arange(10)[None,:].repeat((x.shape[0],1))\n",
        "    mask = torch.ones_like(s)\n",
        "    mask[batches,y] = 0\n",
        "    s = s[mask.type(torch.bool)].reshape(x.shape[0],9)\n",
        "    idxs = torch.randint(0,9,(x.shape[0],))\n",
        "    samples = s[batches,idxs]\n",
        "    y_onehot_fake = torch.zeros((x.shape[0],10),dtype=x.dtype,device=x.device)\n",
        "    y_onehot_fake[batches,samples] = 1.0\n",
        "    x_fake[batches,:10] = y_onehot_fake\n",
        "    return x_real, x_fake\n",
        "\n",
        "def prepare_data_test(x:torch.Tensor, label):\n",
        "    x_test = x.clone()\n",
        "    y_onehot = torch.zeros((x.shape[0],10),dtype=x.dtype,device=x.device)\n",
        "    batches = torch.arange(x.shape[0])\n",
        "    y_onehot[batches,label] = 1.0\n",
        "    x_test[batches, :10] = y_onehot\n",
        "    return x_test\n",
        "\n",
        "# This class acts our baseline layer\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, device, threshold=2.0, epochs=1000):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(input_dim,output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.threshold = threshold\n",
        "        self.epochs = epochs\n",
        "        self.optim = torch.optim.Adam(self.parameters(),lr=0.03)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,x:torch.Tensor):\n",
        "        x = x / (x.norm(p=2,dim=1,keepdim=True)+1e-4)\n",
        "        x = self.lin(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    def forward_forward(self,x_pos, x_neg):\n",
        "        for epoch in (range(self.epochs)):\n",
        "            h_pos = self.forward(x_pos)\n",
        "            h_neg = self.forward(x_neg)\n",
        "            good_pos = (self.threshold-h_pos**2).mean(dim=1)\n",
        "            neg_pos = (h_neg**2-self.threshold).mean(dim=1)\n",
        "            self.optim.zero_grad()\n",
        "            loss = torch.log(1+torch.exp(torch.cat([good_pos,neg_pos]))).mean()\n",
        "            loss.backward()\n",
        "            self.optim.step()\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'epoch {epoch}: loss = {loss}')\n",
        "        return self.forward(x_pos).detach(),self.forward(x_neg).detach()\n",
        "\n",
        "    def predict(self,h:torch.Tensor):\n",
        "        h = h / h.norm(p=2,dim=1,keepdim=True)\n",
        "        h = self.lin(h)\n",
        "        h = self.relu(h)\n",
        "        goodness = (h**2-self.threshold).mean(dim=1)\n",
        "        return h, goodness\n",
        "\n",
        "\n",
        "# this class define entire model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, device, threshold=2.0, epochs=100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.layers = []\n",
        "        self.num_layers = len(hidden_dims) + 1\n",
        "        self.epochs = epochs\n",
        "        assert len(hidden_dims) >= 1\n",
        "        self.layers.append(Layer(input_dim,hidden_dims[0],device,threshold).to(device))\n",
        "        for i in range(1,len(hidden_dims)):\n",
        "            self.layers.append(Layer(hidden_dims[i-1],hidden_dims[i],device,threshold).to(device))\n",
        "\n",
        "    def train(self,trainLoader):\n",
        "        x,y = next(iter(trainLoader))\n",
        "        x_pos, x_neg = prepare_data(x,y)\n",
        "        h_pos = x_pos.to(self.device)\n",
        "        h_neg = x_neg.to(self.device)\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i+1}:\")\n",
        "            #for x,y in tqdm(trainLoader):\n",
        "            h_pos, h_neg = layer.forward_forward(h_pos,h_neg)\n",
        "    def test(self,testLoader,name):\n",
        "        accuracy = 0.0\n",
        "        x, label = next(iter(testLoader))\n",
        "        #for (x,label) in testLoader:\n",
        "        goodness_targets = []\n",
        "        for i in range(10):\n",
        "            z = prepare_data_test(x,i)\n",
        "            z = z.to(device)\n",
        "            goodness_layers = []\n",
        "            for j,layer in enumerate(self.layers):\n",
        "                z, goodness = layer.predict(z)\n",
        "                goodness_layers.append(goodness[:,None])\n",
        "\n",
        "            goodness_targets.append(torch.cat(goodness_layers,dim=1).mean(dim=1,keepdim=True))\n",
        "\n",
        "        goodness_targets = torch.cat(goodness_targets,dim=1)\n",
        "        max_index = goodness_targets.argmax(dim=1)\n",
        "        accuracy = (max_index.cpu()==label).sum().item()/len(label)\n",
        "        print(f\"Accuracy of Model on {name}: {accuracy*100}%\")"
      ],
      "metadata": {
        "id": "1oXx-sO6hFeY"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST Dataset\n",
        "transform = Compose([ToTensor(), Lambda(lambda x: torch.flatten(x))])\n",
        "trainset = MNIST('.',train=True,download=True,transform=transform)\n",
        "testset = MNIST('.',train=False, download=True,transform=transform)\n",
        "\n",
        "batch_size = 60000\n",
        "trainLoader = DataLoader(trainset,batch_size,shuffle=True)\n",
        "testLoader = DataLoader(testset,batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "CV1Lr0xsj5-F"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRain Model\n",
        "input_dim = 784\n",
        "hidden_dims = [512,512]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MLP(input_dim, hidden_dims,device,epochs=20).to(device)\n",
        "model.train(trainLoader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pfomU29l1Bg",
        "outputId": "538a15f3-c457-457c-9501-326242ce3fca"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1:\n",
            "epoch 0: loss = 1.1267484426498413\n",
            "epoch 100: loss = 0.6906740665435791\n",
            "epoch 200: loss = 0.6093816757202148\n",
            "epoch 300: loss = 0.5042091012001038\n",
            "epoch 400: loss = 0.4367710053920746\n",
            "epoch 500: loss = 0.3928852379322052\n",
            "epoch 600: loss = 0.36190348863601685\n",
            "epoch 700: loss = 0.33817917108535767\n",
            "epoch 800: loss = 0.3184700310230255\n",
            "epoch 900: loss = 0.3012816309928894\n",
            "Layer 2:\n",
            "epoch 0: loss = 1.1266560554504395\n",
            "epoch 100: loss = 0.4548075497150421\n",
            "epoch 200: loss = 0.3663649260997772\n",
            "epoch 300: loss = 0.3197694718837738\n",
            "epoch 400: loss = 0.2913164794445038\n",
            "epoch 500: loss = 0.2713039815425873\n",
            "epoch 600: loss = 0.2559657692909241\n",
            "epoch 700: loss = 0.24394433200359344\n",
            "epoch 800: loss = 0.23424845933914185\n",
            "epoch 900: loss = 0.22619059681892395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Model\n",
        "model.test(trainLoader,'train set')\n",
        "model.test(testLoader,'test set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhVcvrwo15OQ",
        "outputId": "78141d45-f748-4070-c7bb-83ebe2a1490c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Model on train set: 91.55499999999999%\n",
            "Accuracy of Model on test set: 91.75999999999999%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تابع هزینه به شیوه تعریف شده خوب است چون در یکی از ترم های آن برای بیشتری شدن آستانه نسبت به داده درست هزینه قرارده شده و به صورت معکوس آن برای داده غلط. در نتیجه با کمک شدن این هزینه دقت شبمه بالا میرود. همچنین برای قرار برچسب با خود اده آن را به بردار onehot تبدیل میکنیم سپس 10 پیکسل اول تصویر را با این بردار جایگزین میکنیم چون 10 پیکسل اول عموما صفر هستند تاثیری ندارد. هرچنید این مدل interpretabel نیست به دلیل تعریف کردین این داده ها ولی دقت خوبی دارد."
      ],
      "metadata": {
        "id": "ycZua55yqBZ9"
      }
    }
  ]
}
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:45:18.620661Z","iopub.status.busy":"2024-02-03T16:45:18.620301Z","iopub.status.idle":"2024-02-03T16:45:30.433579Z","shell.execute_reply":"2024-02-03T16:45:30.432600Z","shell.execute_reply.started":"2024-02-03T16:45:18.620622Z"},"id":"5CRf3OdcabGl","trusted":true},"outputs":[],"source":["!pip install peft"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from peft import LoraConfig, get_peft_model, TaskType\n","from transformers import BertTokenizer, BertConfig, BertModel, AdamW, get_constant_schedule_with_warmup\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from torchmetrics.functional import f1_score, accuracy\n","from tqdm import tqdm\n","import pickle\n","import random\n","import re\n","import nltk\n","import subprocess\n","from nltk.corpus import stopwords\n","from nltk.tokenize import wordpunct_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize.treebank import TreebankWordDetokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T15:13:41.004531Z","iopub.status.busy":"2024-02-03T15:13:41.003613Z","iopub.status.idle":"2024-02-03T15:13:41.013248Z","shell.execute_reply":"2024-02-03T15:13:41.011940Z","shell.execute_reply.started":"2024-02-03T15:13:41.004451Z"},"trusted":true},"outputs":[],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T15:13:41.015816Z","iopub.status.busy":"2024-02-03T15:13:41.015055Z","iopub.status.idle":"2024-02-03T15:13:41.023504Z","shell.execute_reply":"2024-02-03T15:13:41.022235Z","shell.execute_reply.started":"2024-02-03T15:13:41.015772Z"},"trusted":true},"outputs":[],"source":["import gdown\n","\n","gdown.download(\"https://drive.google.com/file/d/1k5LMwmYF7PF-BzYQNE2ULBae79nbM268/view?usp=drive_link\", \"subtaskB_train.jsonl\", quiet=False, fuzzy=True)\n","gdown.download(\"https://drive.google.com/file/d/1oh9c-d0fo3NtETNySmCNLUc6H1j4dSWE/view?usp=drive_link\", \"subtaskB_dev.jsonl\", quiet=False, fuzzy=True)"]},{"cell_type":"markdown","metadata":{"id":"jHC2tIpJabGn"},"source":["### Parameters"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:46:04.437004Z","iopub.status.busy":"2024-02-03T16:46:04.436462Z","iopub.status.idle":"2024-02-03T16:46:04.443626Z","shell.execute_reply":"2024-02-03T16:46:04.442210Z","shell.execute_reply.started":"2024-02-03T16:46:04.436974Z"},"id":"cnf-N9TjabGo","trusted":true},"outputs":[],"source":["batch_size = 64\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","max_length = 128\n","epoch_nums = 3\n","lr = 1e-4\n","epsilon = 1e-8\n","splits = [0.01, 0.05, 0.1, 0.5]\n","\n","train_path = '/kaggle/working/subtaskB_train.jsonl'\n","val_path = '/kaggle/working/subtaskB_dev.jsonl'\n","\n","discriminator_save_path = 'discriminator_G2.pth'\n","generator_save_path = 'generator_G2.pth'\n","bert_save_path = 'bert_G2.pth'\n","report_path = 'report_GAN-BERT_G2.csv'"]},{"cell_type":"markdown","metadata":{"id":"nb_ACjyjabGo"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:46:08.681785Z","iopub.status.busy":"2024-02-03T16:46:08.681442Z","iopub.status.idle":"2024-02-03T16:46:11.319631Z","shell.execute_reply":"2024-02-03T16:46:11.318836Z","shell.execute_reply.started":"2024-02-03T16:46:08.681761Z"},"id":"jN2JnUg7abGo","trusted":true},"outputs":[],"source":["train_data = pd.read_json(train_path,lines=True)\n","val_data = pd.read_json(val_path, lines=True)\n","\n","label_dict = {'chatGPT':0, 'human':1, 'cohere':2, 'davinci':3, 'bloomz':4, 'dolly':5}\n","label2int = lambda label: label_dict[label]\n","\n","train_text = list(train_data['text'])\n","label_train = list(train_data['model'].apply(label2int))\n","text_val= list(val_data['text'])\n","label_val = list(val_data['model'].apply(label2int))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:46:22.968394Z","iopub.status.busy":"2024-02-03T16:46:22.968017Z","iopub.status.idle":"2024-02-03T16:46:24.249897Z","shell.execute_reply":"2024-02-03T16:46:24.248963Z","shell.execute_reply.started":"2024-02-03T16:46:22.968363Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /kaggle/working/...\n","Archive:  /kaggle/working/corpora/wordnet.zip\n","   creating: /kaggle/working/corpora/wordnet/\n","  inflating: /kaggle/working/corpora/wordnet/lexnames  \n","  inflating: /kaggle/working/corpora/wordnet/data.verb  \n","  inflating: /kaggle/working/corpora/wordnet/index.adv  \n","  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n","  inflating: /kaggle/working/corpora/wordnet/index.verb  \n","  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n","  inflating: /kaggle/working/corpora/wordnet/data.adj  \n","  inflating: /kaggle/working/corpora/wordnet/index.adj  \n","  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n","  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n","  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n","  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n","  inflating: /kaggle/working/corpora/wordnet/README  \n","  inflating: /kaggle/working/corpora/wordnet/index.sense  \n","  inflating: /kaggle/working/corpora/wordnet/data.noun  \n","  inflating: /kaggle/working/corpora/wordnet/data.adv  \n","  inflating: /kaggle/working/corpora/wordnet/index.noun  \n","  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","[nltk_data] Downloading package stopwords to /kaggle/working/...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Archive:  /kaggle/working/corpora/stopwords.zip\n"]},{"name":"stderr","output_type":"stream","text":["replace /kaggle/working/corpora/stopwords/dutch? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n","(EOF or read error, treating as \"[N]one\" ...)\n"]}],"source":["# Download and unzip wordnet\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","# Now you can import the NLTK resources as usual\n","from nltk.corpus import wordnet\n","\n","# Download and unzip stopwords\n","try:\n","    nltk.data.find('stopwords.zip')\n","except:\n","    nltk.download('stopwords', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/stopwords.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","# Now you can import the NLTK resources as usual\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:46:29.880677Z","iopub.status.busy":"2024-02-03T16:46:29.880332Z","iopub.status.idle":"2024-02-03T16:46:29.888055Z","shell.execute_reply":"2024-02-03T16:46:29.886945Z","shell.execute_reply.started":"2024-02-03T16:46:29.880650Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text, lemmatizer, stop_words):\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text = text.lower()\n","    text = wordpunct_tokenize(text)\n","    text = [lemmatizer.lemmatize(token) for token in text]\n","    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n","    text = [re.sub(r'\\b[0-9]+\\b', '<NUM>', token) for token in text]\n","    text = [token for token in text if token != '<NUM>']\n","    text = [token for token in text if token not in stop_words]\n","    return TreebankWordDetokenizer().detokenize(text)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:46:41.372349Z","iopub.status.busy":"2024-02-03T16:46:41.371977Z","iopub.status.idle":"2024-02-03T17:05:57.075487Z","shell.execute_reply":"2024-02-03T17:05:57.073808Z","shell.execute_reply.started":"2024-02-03T16:46:41.372320Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad3cee2a4f564a34b3a79309b5251197","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffd1d17e81b84862a2d7ec8a71fc68ef","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10d4f5b3caab4a46b998b168ea24026a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7927517327984a6bba373cbfc926b310","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 71027/71027 [18:11<00:00, 65.08it/s] \n","100%|██████████| 3000/3000 [01:02<00:00, 47.85it/s]\n"]}],"source":["lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","bow = torch.tensor([0], dtype = torch.long)\n","for text in tqdm(train_text):\n","    text = preprocess_text(text, lemmatizer, stop_words)\n","    tokens = tokenizer(text, max_length=max_length, add_special_tokens=False, truncation=False, padding=False, return_tensors='pt')\n","    bow = torch.cat((bow, tokens['input_ids'].view(-1)))\n","    \n","for text in tqdm(text_val):\n","    text = preprocess_text(text, lemmatizer, stop_words)\n","    tokens = tokenizer(text, max_length=max_length, add_special_tokens=False, truncation=False, padding=False, return_tensors='pt')\n","    bow = torch.cat((bow, tokens['input_ids'].view(-1)))\n","\n","bow = bow[1:]    \n","with open('bow_list.pkl','wb') as f:\n","    pickle.dump(bow, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('/kaggle/working/bow_list.pkl','rb') as f:\n","    bow = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-02T16:37:53.957143Z","iopub.status.idle":"2024-02-02T16:37:53.958204Z","shell.execute_reply":"2024-02-02T16:37:53.957975Z","shell.execute_reply.started":"2024-02-02T16:37:53.957956Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","splits = [0.01, 0.05, 0.1, 0.5]\n","train_datasets = []\n","for split in splits:\n","    labeled_text, unlabeled_text, label, _  = train_test_split(train_text,label_train,test_size=1-split)\n","    label = torch.tensor(label)\n","    tokenized_labeled_text = tokenizer(labeled_text, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt')\n","    tokenized_unlabeled_text = tokenizer(unlabeled_text, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt')\n","    rep_factor = int(np.log(len(unlabeled_text)/len(labeled_text)))\n","    if split == 0.5:\n","        rep_factor = 1\n","        \n","    tokenized_text = {'input_ids':torch.cat([tokenized_labeled_text['input_ids'].repeat((rep_factor,1)),tokenized_unlabeled_text['input_ids']],dim=0),\n","                      'attention_mask': torch.cat([tokenized_labeled_text['attention_mask'].repeat((rep_factor,1)),tokenized_unlabeled_text['attention_mask']],dim=0),\n","                      'label': torch.cat([label.repeat(rep_factor),-torch.ones(len(unlabeled_text))])}\n","    \n","    train_dataset = TensorDataset(tokenized_text['input_ids'],tokenized_text['attention_mask'], tokenized_text['label'].type(torch.int32))\n","    train_datasets.append(train_dataset)\n","    print(f\"train dataset for split {split} added.\")\n","\n","with open('train_datasets.pkl','wb') as f:\n","     pickle.dump(train_datasets,f)\n","\n","tokenized_text = tokenizer(text_val, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt')\n","val_dataset = TensorDataset(tokenized_text['input_ids'], tokenized_text['attention_mask'], torch.tensor(label_val).type(torch.int32))\n","with open('val_dataset.pkl','wb') as f:\n","     pickle.dump(val_dataset,f)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:55:45.963596Z","iopub.status.busy":"2024-02-02T21:55:45.962876Z","iopub.status.idle":"2024-02-02T21:55:47.680513Z","shell.execute_reply":"2024-02-02T21:55:47.679720Z","shell.execute_reply.started":"2024-02-02T21:55:45.963564Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/working/train_datasets.pkl','rb') as f:\n","    train_datasets = pickle.load(f)\n","\n","with open('/kaggle/working/val_dataset.pkl','rb') as f:\n","    val_dataset = pickle.load(f)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:55:51.503977Z","iopub.status.busy":"2024-02-02T21:55:51.503624Z","iopub.status.idle":"2024-02-02T21:55:51.509540Z","shell.execute_reply":"2024-02-02T21:55:51.508378Z","shell.execute_reply.started":"2024-02-02T21:55:51.503950Z"},"id":"Jm-UYZjPabGp","trusted":true},"outputs":[],"source":["trainLoaders = []\n","for train_dataset in train_datasets:\n","    trainLoaders.append(DataLoader(train_dataset,batch_size=batch_size,shuffle=True))\n","\n","valLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"ahQZLD-uabGp"},"source":["### Model"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:04:39.404805Z","iopub.status.busy":"2024-02-03T20:04:39.404439Z","iopub.status.idle":"2024-02-03T20:04:39.416898Z","shell.execute_reply":"2024-02-03T20:04:39.415208Z","shell.execute_reply.started":"2024-02-03T20:04:39.404776Z"},"id":"jXobehoRabGp","trusted":true},"outputs":[],"source":["class Generator1(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(nn.Linear(100,768), nn.LeakyReLU(), nn.Dropout(p=0.1), nn.Linear(768,768))\n","\n","    def forward(self):\n","        epsilon = torch.randn(batch_size,100).cuda()\n","        return self.model(epsilon)\n","\n","class Generator2(nn.Module):\n","    def __init__(self, bow, input_size):\n","        super().__init__()\n","        self.model = Bert()\n","        self.bow = bow\n","        self.input_size = input_size\n","\n","    def forward(self):\n","        indices = torch.randint(0, len(self.bow), (batch_size,self.input_size))\n","        samples = self.bow[indices]\n","        return self.model(samples.long(), torch.ones((batch_size, self.input_size), dtype=torch.long))\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.feat = nn.Sequential(nn.Dropout(p=0.1), nn.Linear(768,768), nn.LeakyReLU(), nn.Dropout(p=0.1))\n","        self.logit = nn.Linear(768,7)\n","\n","    def forward(self, x):\n","        feat = self.feat(x)\n","        logit = self.logit(feat)\n","        return feat, logit\n","\n","class Bert(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","        lora_config = LoraConfig(\n","        task_type=TaskType.FEATURE_EXTRACTION, # this is necessary\n","        )\n","\n","        # add LoRA adaptor\n","        self.model = get_peft_model(self.model, lora_config)\n","\n","    def forward(self, input_ids, att_mask):\n","        return self.model(input_ids, att_mask)[0][:,0,:]"]},{"cell_type":"markdown","metadata":{"id":"pjB_MTKAabGq"},"source":["### Training and Validation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:57:02.381605Z","iopub.status.busy":"2024-02-02T21:57:02.380786Z","iopub.status.idle":"2024-02-02T21:57:02.390615Z","shell.execute_reply":"2024-02-02T21:57:02.389555Z","shell.execute_reply.started":"2024-02-02T21:57:02.381573Z"},"id":"41Vg4jpdabGq","trusted":true},"outputs":[],"source":["class GAN_Bert_Loss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, logits_bert, targets, logits_gen, feat_bert, feat_G):\n","        # logits have shape B x 7\n","        # targets has shape B\n","        probs_G = torch.nn.functional.softmax(logits_gen,dim=-1)\n","        lossG_unsup = -torch.mean(torch.log(1-probs_G[:,-1]+epsilon))\n","        lossG_feat = torch.mean(torch.pow(feat_G.mean(dim=0) - feat_bert.mean(dim=0),2))\n","\n","        logits_bert_labeled = logits_bert[:,0:-1] # has shape B x 6\n","        label_mask = targets != -1\n","        lossD_sup = 0\n","        if label_mask.sum()>0:\n","            lossD_sup = self.criterion(logits_bert[:,0:-1][label_mask], targets[label_mask])\n","\n","        probs_bert = torch.nn.functional.softmax(logits_bert,dim=-1)\n","        probs_G_detached = probs_G.detach()\n","        lossD_unsup = -torch.mean(torch.log(1-probs_bert[:,-1]+epsilon)) - torch.mean(torch.log(probs_G_detached[:,-1]+epsilon))\n","\n","\n","        lossD = lossD_sup + lossD_unsup\n","        lossG = lossG_feat + lossG_unsup\n","        \n","        return lossD, lossG"]},{"cell_type":"markdown","metadata":{},"source":["### Training G1"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:57:04.529209Z","iopub.status.busy":"2024-02-02T21:57:04.528845Z","iopub.status.idle":"2024-02-02T21:57:04.537003Z","shell.execute_reply":"2024-02-02T21:57:04.536033Z","shell.execute_reply.started":"2024-02-02T21:57:04.529180Z"},"id":"Z9BuWvleabGq","trusted":true},"outputs":[],"source":["def validation(bert, discriminator, valLoader):\n","    with torch.no_grad():\n","        bert.eval()\n","        discriminator.eval()\n","        all_prediction = []\n","        all_targets = []\n","        for i, batch in tqdm(enumerate(valLoader), total=len(valLoader), desc=f'Validation'):\n","\n","            input_ids = batch[0].cuda()\n","            att_mask = batch[1].cuda()\n","            targets = batch[2].type(torch.long).cuda()\n","\n","            y_bert = bert(input_ids, att_mask)\n","            feat_bert, logit_bert = discriminator(y_bert)\n","\n","            preds = logit_bert[:,0:-1].max(dim=-1)[1]\n","            all_prediction.append(preds.cpu())\n","            all_targets.append(targets.cpu())\n","\n","\n","    return f1_score(preds, targets, 'multiclass', num_classes=6), accuracy(preds, targets, 'multiclass', num_classes=6)"]},{"cell_type":"markdown","metadata":{},"source":["### Training G2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1s = []\n","accs = []\n","\n","for split, trainLoader  in zip(splits,trainLoaders):\n","    generator = Generator1().cuda()\n","    discriminator = Discriminator().cuda()\n","    bert = Bert().cuda()\n","\n","    bert = torch.nn.parallel.DataParallel(bert, device_ids=list(range(2)), dim=0)\n","    generator = torch.nn.parallel.DataParallel(generator, device_ids=list(range(2)), dim=0)\n","    discriminator = torch.nn.parallel.DataParallel(discriminator, device_ids=list(range(2)), dim=0)\n","\n","    criterion = GAN_Bert_Loss().cuda()\n","    gen_optimizer = AdamW(list(generator.parameters()), lr=lr)\n","    dis_optimizer = AdamW(list(bert.parameters())+list(discriminator.parameters()), lr=lr)\n","    \n","    num_train_steps = int(len(trainLoader) * epoch_nums)\n","    num_warmup_steps = int(num_train_steps * 0.1)\n","    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n","    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n","    \n","    for epoch in range(epoch_nums):\n","        generator.train()\n","        discriminator.train()\n","        bert.train()\n","        loss_gen = 0.0\n","        loss_dis = 0.0\n","        \n","        for i, batch in tqdm(enumerate(trainLoader), total=len(trainLoader), desc=f'({split}) epoch {epoch}'):\n","\n","            input_ids = batch[0].cuda()\n","            att_mask = batch[1].cuda()\n","            targets = batch[2].type(torch.long).cuda()\n","\n","            y_bert = bert(input_ids, att_mask)\n","            y_gen = generator()\n","            feat_G, logit_G = discriminator(y_gen)\n","            feat_bert, logit_bert = discriminator(y_bert)\n","\n","            lossD, lossG = criterion(logit_bert, targets, logit_G, feat_bert, feat_G)\n","\n","            gen_optimizer.zero_grad()\n","            lossG.backward(retain_graph=True)\n","            gen_optimizer.step()\n","\n","            dis_optimizer.zero_grad()\n","            lossD.backward()\n","            dis_optimizer.step()\n","\n","            loss_gen += lossG.item()\n","            loss_dis += lossD.item()\n","\n","            scheduler_d.step()\n","            scheduler_g.step()\n","\n","        print(f'loss: {(loss_gen+loss_dis)/len(trainLoader)}, Generator Loss: {loss_gen / len(trainLoader)}, Discriminator Loss: {loss_dis/len(trainLoader)}')\n","        f1, acc = validation(bert, discriminator, valLoader)\n","        print(f'f1 score: {f1.item()}, accuracy: {acc.item()}')\n","\n","        torch.save(discriminator.state_dict(), f'split_{split}_'+discriminator_save_path)\n","        torch.save(generator.state_dict(), f'split_{split}_'+generator_save_path)\n","        torch.save(bert.state_dict(), f'split_{split}_'+bert_save_path)\n","\n","    f1s.append(f1.item())\n","    accs.append(acc.item())\n","\n","report = pd.DataFrame({\"splits\": splits, \"accuracies\": accs, \"f1 score\": f1s})\n","report.to_csv(report_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"execution":{"iopub.execute_input":"2024-02-02T21:57:07.962542Z","iopub.status.busy":"2024-02-02T21:57:07.961912Z"},"id":"DX_cJ-6GabGr","outputId":"4a8df9af-cf41-472b-939e-dc89f408ffab","trusted":true},"outputs":[],"source":["f1s = []\n","accs = []\n","\n","for split, trainLoader  in zip(splits,trainLoaders):\n","    generator = Generator2(bow, 128).cuda()\n","    discriminator = Discriminator().cuda()\n","    bert = Bert().cuda()\n","\n","    bert = torch.nn.parallel.DataParallel(bert, device_ids=list(range(2)), dim=0)\n","    generator = torch.nn.parallel.DataParallel(generator, device_ids=list(range(2)), dim=0)\n","    discriminator = torch.nn.parallel.DataParallel(discriminator, device_ids=list(range(2)), dim=0)\n","\n","    criterion = GAN_Bert_Loss().cuda()\n","    gen_optimizer = AdamW(list(generator.parameters()), lr=lr)\n","    dis_optimizer = AdamW(list(bert.parameters())+list(discriminator.parameters()), lr=lr)\n","    \n","    num_train_steps = int(len(trainLoader) * epoch_nums)\n","    num_warmup_steps = int(num_train_steps * 0.1)\n","    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n","    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n","    \n","    for epoch in range(epoch_nums):\n","        generator.train()\n","        discriminator.train()\n","        bert.train()\n","        loss_gen = 0.0\n","        loss_dis = 0.0\n","        \n","        for i, batch in tqdm(enumerate(trainLoader), total=len(trainLoader), desc=f'({split}) epoch {epoch}'):\n","\n","            input_ids = batch[0].cuda()\n","            att_mask = batch[1].cuda()\n","            targets = batch[2].type(torch.long).cuda()\n","\n","            y_bert = bert(input_ids, att_mask)\n","            y_gen = generator()\n","            feat_G, logit_G = discriminator(y_gen)\n","            feat_bert, logit_bert = discriminator(y_bert)\n","\n","            lossD, lossG = criterion(logit_bert, targets, logit_G, feat_bert, feat_G)\n","\n","            gen_optimizer.zero_grad()\n","            lossG.backward(retain_graph=True)\n","            gen_optimizer.step()\n","\n","            dis_optimizer.zero_grad()\n","            lossD.backward()\n","            dis_optimizer.step()\n","\n","            loss_gen += lossG.item()\n","            loss_dis += lossD.item()\n","\n","            scheduler_d.step()\n","            scheduler_g.step()\n","\n","        print(f'loss: {(loss_gen+loss_dis)/len(trainLoader)}, Generator Loss: {loss_gen / len(trainLoader)}, Discriminator Loss: {loss_dis/len(trainLoader)}')\n","        f1, acc = validation(bert, discriminator, valLoader)\n","        print(f'f1 score: {f1.item()}, accuracy: {acc.item()}')\n","\n","        torch.save(discriminator.state_dict(), f'split_{split}_'+discriminator_save_path)\n","        torch.save(generator.state_dict(), f'split_{split}_'+generator_save_path)\n","        torch.save(bert.state_dict(), f'split_{split}_'+bert_save_path)\n","\n","    f1s.append(f1.item())\n","    accs.append(acc.item())\n","\n","report = pd.DataFrame({\"splits\": splits, \"accuracies\": accs, \"f1 score\": f1s})\n","report.to_csv(report_path)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4393351,"sourceId":7544237,"sourceType":"datasetVersion"},{"datasetId":4393655,"sourceId":7544715,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
